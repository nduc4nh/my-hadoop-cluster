# APACHE HADOOP CLUSTER & HADOOP ECO

**This includes:**
* HDFS
* YARN
* HIVE
* Spark

## PROJECT STRUCTURE

For more detail, please refer to more readme files inside each folder listed below
```
__/
  |__hadoop
  |       |____logs
  |       |
  |       |____README.md
  |
  |__hdfs
  |     |____namenode
  |     |
  |     |____datanode
  |
  |
  |__yarn
  |     |____logs
  |     |
  |     |____README.md
  |
  |__hive
  |     |____README.md
  |
  |
  |__downloads
  |          |____README.md
  |
  |__spark
          \____README.md
```

**hadoop**: includes hadoop setup, config files

**hdfs**: includes namenode and datanode directory

**yarn**: includes yarn files

**hive**: includes hive setup. config files

**downloads**: includes all installation files (e.g hadoop, hive, ..) 

**README**: guidelines

## Requirements:

* [Apache Hadoop](https://hadoop.apache.org/releases.html) (A)

* [Hive](https://dlcdn.apache.org/hive/) (B)

* [Spark](https://spark.apache.org/downloads.html) (B)

**Note**: *Hadoop must be installed first before taking any action related to other tools*

## References:

* [Hadoop setup](https://github.com/nduc4nh/my-hadoop-cluster/tree/main/hadoop)
* [Hive setup](https://github.com/nduc4nh/my-hadoop-cluster/tree/main/hive)
* [Spark](https://github.com/nduc4nh/my-hadoop-cluster/tree/main/spark)
